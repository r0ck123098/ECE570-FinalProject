{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2a632d-bd52-4b2b-ac14-bd7f38091309",
   "metadata": {},
   "source": [
    "Since I am doing the NLP and none of my 3 paper give me a github repo to reference so I am using the BERT pre-trained model to help with me. I have the own training method and also I am using the trainer function to help with the fine-tune of my model. And since BERT is a pre-trained model that trained with a insanely huge datasest, so when I am training on specific small set, some responses are really hard to change from the pre-trained model. And due to the limitation of computer resource and dataset size, the performance of the intended BERT may not be that ideal, so I compare the difference of output on BERT to implies that when we get enough dataset and computer source the model will work as intended.\n",
    "<br> For all of the model, you can download them here, so you don't need to wait for days for them to finish.\n",
    "<br> Important: Remember to unzip.\n",
    "<br> https://drive.google.com/drive/folders/1xdxUoJPUHNZuCw4ieboaLBfCa2kg1sPg?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8c9caf-1f4a-4995-9a41-0b21aabb9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480c37d2-a33e-47dc-9f45-897abc5acf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling, BertForMaskedLM, BertForNextSentencePrediction, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43450e65-6a50-44ff-8951-87c7895a9a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I can use GPU now? -- True \n",
      "The Graphics Card Model is NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "# Check If I can use GPU or not\n",
    "print(f'Can I can use GPU now? -- {torch.cuda.is_available()} \\nThe Graphics Card Model is {torch.cuda.get_device_name(0)}')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83362500-377d-4907-8c0d-34ae7fd0c05c",
   "metadata": {},
   "source": [
    "This Part below is pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f576ec37-f743-4a8e-ac95-757b27b5ff9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load bert model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959fa44e-46c8-4606-94d6-4680844a7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(model_use, sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    print(inputs.input_ids)\n",
    "    inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_use(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
    "    predicted_word = tokenizer.decode(predicted_token_id)\n",
    "    print(f\"Original sentence: {sentence}\")\n",
    "    print(f\"Predicted word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d32a80-67b8-4bdb-8157-e12c805858d5",
   "metadata": {},
   "source": [
    "Some explanation on the input_ids, for the tensor with value 101, it is [CLS] means the start of the sentence, for the tensor with value 102 it is [SEP] means the sentence separator, for the tensor with value 103, it is [MASK], which is the word being hidden. And that is what we are going to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e6dbe5-f3b2-4fda-9c79-3989401413c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2572, 3407, 2651, 2061, 1045, 2097,  103, 2678, 2399, 1012,\n",
      "          102]])\n",
      "Original sentence: I am happy today so I will [MASK] video games.\n",
      "Predicted word: play\n"
     ]
    }
   ],
   "source": [
    "# test with the pre-trained model\n",
    "sentence = \"I am happy today so I will [MASK] video games.\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3802df1-e54b-4465-b74d-7676ec012bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1996, 2837, 2787, 2000,  103, 1996, 6378, 2076, 1996, 5219, 1012,\n",
      "          102]])\n",
      "Original sentence: The committee decided to [MASK] the proposal during the session.\n",
      "Predicted word: consider\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The committee decided to [MASK] the proposal during the session.\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e08159-49c0-4f20-97c3-f2be3f837677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1996, 7155, 8704, 2000,  103, 1037, 6748, 4824, 1997, 1996, 9575,\n",
      "         1012,  102]])\n",
      "Original sentence: The scientist aims to [MASK] a deeper understanding of the phenomenon.\n",
      "Predicted word: gain\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The scientist aims to [MASK] a deeper understanding of the phenomenon.\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30513a00-af46-4a00-a5e7-563593b2135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2076,  1996,  2397,  3955,  1010,  3488,   103,  2000,  3443,\n",
      "          1037,  2047,  3307,  4957,  2006,  1996,  2148,  2217,  1997,  2697,\n",
      "         21934, 16288,  2000,  7532,  3307,  4278,  1998,  3307,  2260,   102]])\n",
      "Original sentence: During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\n",
      "Predicted word: developed\n"
     ]
    }
   ],
   "source": [
    "sentence = \"During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46faf1fe-fb47-4bcf-8aab-6260dd48322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets, I use wikitext here for now\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06eb58c-1d77-4983-8388-951fa82508fd",
   "metadata": {},
   "source": [
    "For this part of the code, I use the trainer utilities for easier training, I will have my own implementation of train on next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8932a8e9-0958-451b-803e-d2831fe2bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b1d0ae-ce4f-4f83-be0c-dad8756a6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "# I have trained the model and upload it to the google drive, you can download it and run it from here, you might don't want to run the code for 14 mins\n",
    "# The training loss output is lost because I rerun this cell, you can find it in the final term paper report.\n",
    "# You can uncomment the this code if u wish to train, it takes about 35mins for my pc to run.\n",
    "# Also the num of epoch is 10 for this one\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6bc72e8-c165-4ac8-85eb-927014bb59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_history = trainer.state.log_history\n",
    "# for entry in training_history:\n",
    "#     if \"loss\" in entry:\n",
    "#         print(f\"Step {entry['step']}: Loss = {entry['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e298102-603b-48ad-8fcb-e57079bea781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./mlm_trained_model\");\n",
    "# tokenizer.save_pretrained(\"./mlm_trained_model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea7cd2-20cf-4826-ac64-4213a2efb09e",
   "metadata": {},
   "source": [
    "Important: It is meaning less to test the accuracy of the MLM, because for missing word of each sentences, it can have multiple choices.\n",
    "For example: The food was really [MASK]. The words \"good\", \"delicious\", \"tasty\" all make sense here. So it is just pointless to measure the accuracy of the BERT MODEL.\n",
    "So, instead, we use the perplexity instead of accuracy. Perplexity is a more commonly used indicator for evaluating the rationality and fluency of language model generation, especially for language modeling tasks. Perplexity measures the model's prediction confidence for all words in the test set, reflecting the model's grasp of the overall language structure and semantics. Compared with a single accuracy rate, perplexity provides a more comprehensive evaluation standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b657cc82-ca6d-487b-aed1-cef88d98504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "# tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "# trainer.eval_dataset = tokenized_eval_dataset\n",
    "# eval_result = trainer.evaluate()\n",
    "# print(eval_result)\n",
    "# loss = eval_result[\"eval_loss\"]\n",
    "# perplexity = math.exp(loss)\n",
    "# print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71aff7-0d46-4d2e-9934-8e744d4772b8",
   "metadata": {},
   "source": [
    "The output of perplexity is also showed in the report directly, if you train the model locally, then you can uncomment to see the perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43003817-4a3f-4555-b94e-2c934b91aa9c",
   "metadata": {},
   "source": [
    "This part below is self-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062d3c3f-e105-4b6f-99c5-bcc5f3e85392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./mlm_trained_model\" \n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddec510a-46b7-48b9-9b86-7980af61a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2572, 3407, 2651, 2061, 1045, 2097,  103, 2208,  102]])\n",
      "Original sentence: I am happy today so I will [MASK] game\n",
      "Predicted word: play\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am happy today so I will [MASK] game\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae63a4e-693a-4408-a1b1-5d27632639a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1996, 7155, 8704, 2000,  103, 1037, 6748, 4824, 1997, 1996, 9575,\n",
      "         1012,  102]])\n",
      "Original sentence: The scientist aims to [MASK] a deeper understanding of the phenomenon.\n",
      "Predicted word: gain\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The scientist aims to [MASK] a deeper understanding of the phenomenon.\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed64a5e-d421-4cef-8317-13c12e2e0072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1996, 2837, 2787, 2000,  103, 1996, 6378, 2076, 1996, 5219, 1012,\n",
      "          102]])\n",
      "Original sentence: The committee decided to [MASK] the proposal during the session.\n",
      "Predicted word: consider\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The committee decided to [MASK] the proposal during the session.\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362948b0-c353-4e9b-ae04-cf26b7cc131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2076,  1996,  2397,  3955,  1010,  3488,   103,  2000,  3443,\n",
      "          1037,  2047,  3307,  4957,  2006,  1996,  2148,  2217,  1997,  2697,\n",
      "         21934, 16288,  2000,  7532,  3307,  4278,  1998,  3307,  2260,   102]])\n",
      "Original sentence: During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\n",
      "Predicted word: began\n"
     ]
    }
   ],
   "source": [
    "sentence = \"During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\"\n",
    "print_prediction(model, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aee2a0-1ce7-41c2-8ece-1435029f0462",
   "metadata": {},
   "source": [
    "This parts below shows my own training function instead of using trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78cabb01-48df-4ef6-97c7-65ad9128df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e346e1d4-b935-4633-ba7c-135e9d95c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63fb2972-a1db-46af-896a-cca1a423cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2295/2295 [02:45<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for epoch 1: 1.9247\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2295/2295 [02:43<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for epoch 2: 1.7795\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2295/2295 [02:41<00:00, 14.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for epoch 3: 1.6701\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-5\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    avg_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    print(f\"Average training loss for epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ffc353-18a9-4d84-a99d-3174edfe730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2076,  1996,  2397,  3955,  1010,  3488,   103,  2000,  3443,\n",
      "          1037,  2047,  3307,  4957,  2006,  1996,  2148,  2217,  1997,  2697,\n",
      "         21934, 16288,  2000,  7532,  3307,  4278,  1998,  3307,  2260,   102]])\n",
      "Original sentence: During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\n",
      "Predicted word: emerged\n"
     ]
    }
   ],
   "source": [
    "sentence = \"During the late 1970s , plans [MASK] to create a new highway link on the south side of Lake Simcoe to connect Highway 400 and Highway 12\"\n",
    "print_prediction(model,sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478247d4-b431-4040-a313-26f1dc1f8d8f",
   "metadata": {},
   "source": [
    "The PART BELOW THIS LINE IS BERT NextSentencePrediction (NSP) \n",
    "<br> This part I directly use trainer instead of implement own training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7b49e7a-7dd8-4a93-a2ca-104a1ad73dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForNextSentencePrediction.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b94a37ed-779c-49d1-ae68-08b8e6694736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A: I went to the store\n",
      "Sentence B: bought some fresh vegetables.\n",
      "Sentence B is likely the next sentence for Sentence A.\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"I went to the store\"\n",
    "sentence_b = \"bought some fresh vegetables.\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "if predicted_label == 0:\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B: {sentence_b}\")\n",
    "    print(\"Sentence B is likely the next sentence for Sentence A.\")\n",
    "else:\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B: {sentence_b}\")\n",
    "    print(\"Sentence B is NOT the next sentence for Sentence A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21e829f6-b0ec-4608-8609-9af2378f7d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A: I am happy\n",
      "Sentence B: blamed me for an hour in school.\n",
      "Sentence B is NOT the next sentence for Sentence A.\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"I am happy\"\n",
    "sentence_b = \"blamed me for an hour in school.\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "if predicted_label == 0:\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B: {sentence_b}\")\n",
    "    print(\"Sentence B is likely the next sentence for Sentence A.\")\n",
    "else:\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B: {sentence_b}\")\n",
    "    print(\"Sentence B is NOT the next sentence for Sentence A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c12aff0-a60a-4fc5-825a-c7a5ee09e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train[:1%]\")\n",
    "def prepare_sentence_pairs(examples):\n",
    "    sentence_pairs = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    texts = examples[\"text\"]\n",
    "    num_texts = len(texts)\n",
    "    all_texts = dataset[\"text\"]\n",
    "    total_texts = len(all_texts)\n",
    "\n",
    "    for i in range(num_texts):\n",
    "        sentence_a = texts[i]\n",
    "\n",
    "        if random.random() < 0.5 and i < num_texts - 1:\n",
    "            sentence_b = texts[i + 1]\n",
    "            label = 0 \n",
    "        else:\n",
    "            random_index = random.randint(0, total_texts - 1)\n",
    "            while random_index == i or (i < num_texts - 1 and random_index == i + 1):\n",
    "                random_index = random.randint(0, total_texts - 1)\n",
    "            sentence_b = all_texts[random_index]\n",
    "            label = 1\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            sentence_a,\n",
    "            sentence_b,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        sentence_pairs[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "        sentence_pairs[\"token_type_ids\"].append(encoded[\"token_type_ids\"])\n",
    "        sentence_pairs[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "        sentence_pairs[\"labels\"].append(label)\n",
    "\n",
    "    return sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89315acf-ba79-4434-852c-72276200194b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(prepare_sentence_pairs, batched=True, remove_columns=[\"text\"]);\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nsp_results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c55d525-daa2-419a-8c0b-a40d0d2a06a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# since the training takes forever and when i restart it, the output of this column is missed\n",
    "# but you can load my model by downloading it directly from the google drive\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "876e4b47-1687-41e2-8d31-1399dbea984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"./nsp_trained_model\"\n",
    "# trainer.save_model(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7546779-43c2-4ded-95d7-7597e3870f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./nsp_trained_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForNextSentencePrediction.from_pretrained(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd15ab-ee9b-4353-89bd-ce52a01d8f94",
   "metadata": {},
   "source": [
    "The perplexity of NSP is actually not that ideal because the dataset is still too small for our local machine to train, the pre-trained model has better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c878dd6d-1848-4006-946e-9ad27d9b33cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence B is likely the next sentence for Sentence A.\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"How old is Tommy?\"\n",
    "sentence_b = \"Tommy loves to eat banana.\"\n",
    "\n",
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "if predicted_label == 0:\n",
    "    print(\"Sentence B is likely the next sentence for Sentence A.\")\n",
    "else:\n",
    "    print(\"Sentence B is NOT the next sentence for Sentence A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca2935c4-8ddd-4970-9c4f-9e18e2333604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence B is likely the next sentence for Sentence A.\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"I am happy\"\n",
    "sentence_b = \"blamed me for an hour in school.\"\n",
    "\n",
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "if predicted_label == 0:\n",
    "    print(\"Sentence B is likely the next sentence for Sentence A.\")\n",
    "else:\n",
    "    print(\"Sentence B is NOT the next sentence for Sentence A.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf08bf7-33da-4210-aad2-15067142d372",
   "metadata": {},
   "source": [
    "This Part of the code is for BERT-BASED-CHINESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7a92bfd-2b78-4b6b-b5e7-6e6a7f83666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-chinese'\n",
    "samples = ['[CLS] ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ [SEP] åŒ—äº¬æ˜¯ [MASK] å›½çš„é¦–éƒ½ã€‚ [SEP]']\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25ac44b0-461a-4d87-891a-8e5c1111eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  704, 1744, 4638, 7674, 6963, 3221, 1525, 7027, 8043,  102, 1266,\n",
      "          776, 3221,  103, 1744, 4638, 7674, 6963,  511,  102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [tokenizer.tokenize(i) for i in samples]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03e5dde6-024c-4a73-a271-355b528770f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 21128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids)\n",
    "prediction_scores = outputs[0]\n",
    "prediction_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55be3aae-18a2-4237-acbf-3312105b966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä¸­'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = prediction_scores[0].detach().numpy()\n",
    "pred = np.argmax(sample, axis=1)\n",
    " \n",
    "tokenizer.convert_ids_to_tokens(pred)[14]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
